\chapter{Utilizing Onboard Compute In SteelEagle}
\label{ch:voxl}

This chapter discusses work to replace the communications relay used in
SteelEagle to the Modal AI VOXL 2. The VOXL 2 offers significant computational
capability, allowing the execution of machine learning models.  We start by
characterizing this shift in offloading strategy in more general terms.

\section{The Computation Offload Spectrum}
\begin{figure}[htbp]
\centerline{\includegraphics[width = .8\textwidth]{figs/offload-spectrum-crop.pdf}}
\caption{Devices Placed in the Computation Offloading Spectrum}
\label{fig:offload-spectrum}
\end{figure}
Devices today leverage offloading to various degrees.
\Cref{fig:offload-spectrum} shows how devices can be placed in an offloading
spectrum, with thin clients found towards the left of the spectrum and thick
clients to the right. A thin client, as opposed to a thick client, has minimal
compute and can be made smaller, lighter, and cheaper. Virtual desktop
infrastructure (VDI), for example, leverages pure offload by providing remote
access to desktops hosted on centralized servers.  This reduces the
computational demands on the client device, enabling the use of
computationally-intensive applications on weaker and older ``thin'' hardware,
reducing costs. Similarly, internet-of-things (IoT) sensors, such as video
cameras, cirumvent their limited storage and computational ability by
leveraging pure offload, transmitting their sensor streams to the cloud for
storage and analysis.

On the other end of the spectrum, we find devices such as gaming consoles,
desktop computers, and autonomous cars. Gaming consoles and desktop computers
are expected to have crisp interaction, which is not achievable by offloading
to cloud. Offloading to cloudlets, which can provide low latency, is not a
feasible option for these devices currently because of the absence of
widespread cloudlet infrastructure.  Gaming consoles and desktop computers can
provide high-levels of onboard computational power because they are not subject
to mobility constraints.  Although autonomous cars are mobile, they typically
have sufficient energy available to perform intensive onboard computations
since the majority of energy is needed to propel the car forward. They must
also perform real-time decision making reliably even in the absence of network
connectivity, which makes offloading less appealing.

In the middle of the spectrum, we find devices that perform partial offloading.
These devices have sufficient computational power to function when offloading
is unavailable or not advantageous, but they also offload computation in other
cases. Smartphone AI voice assistants such as Apple's Siri, for example,
originally offloaded all computation to the cloud. Over time, it evolved to
utilize on-device capabilities to perform simpler tasks, allowing its use in
the absence of an internet connection.  Chromebooks were designed with a focus
on the use of cloud-based services, allowing them to be fitted with lower
processing power, memory, and storage capacity compared to traditional laptops.

\subsection{Deciding the Offloading Strategy}
\label{sec:deciding-offloading-strategy}

Choosing an offload strategy for a given application can be done based on some
key attributes that are considered important for the application:
\begin{itemize}
    \item \textbf{Mobility requirements.} Is the application subject to
        mobility constraints?
    \item \textbf{Disconnected operation.} Does the application need to work reliably
        in the face of network disconnection or degradation? Is degraded operation
        acceptable during disconnected operation?
    \item \textbf{Latency requirements.} Is the application latency-sensitive?
    \item \textbf{Bandwidth constraints.} How much network bandwidth is
        available?
    \item \textbf{Cost requirements.} Is low cost for devices a priority in
        this application?
\end{itemize}

These attributes encapsulate the benefits and limitations of offloading.
Offloading is most beneficial for mobile devices. A mobile device that
is dependent entirely on offloading will be unable to function adequately when
offloading is impossible altogether because of network disconnection, or
impacted because of degraded network conditions, such as high latency and low
bandwidth availability. Applications that are mission-critical would be wise to
equip devices with sufficient computational resources to support an acceptable
level of function when adverse network conditions impact offloading.
Applications that are not mission-critical can rely on pure offload to benefit
from the cost savings resulting from using devices that are not required to
have significant computational resources.

Offloading allows mobile devices to achieve a much higher level of
functionality that their onboard hardware allows for. Equipping mobile devices
with the hardware needed to maintain the same level of functionality during
offload degradations reduces the benefit of offloading---the mobile device is
weighed down with the additional size and weight of more complex hardware that
remains unused when offloading is possible.

If offload degradations are infrequent, a more reasonable approach is to
utilize the inferior on-device computational resources to compute results that
are inferior according to an output-specific metric, compared to those obtained
using offloading. For predictions from a machine learning model, the metric
could be the accuracy of the prediction. In previous work, the concept of
\textit{fidelity} has been used in this context, defined as the degree to which
the results differ according to the metric \cite{noble1997}.

\subsection{The Dynamic Nature of Partial Offload}
\label{sec:offloading-dynamic-nature}

Network conditions affect the performance of offloading, but they are dynamic
in nature. Mobile devices utilizing partial offload, then, must decide an
offloading strategy at runtime. But the decision is not binary---exclusively
using onboard compute or remote compute resources.  As
\cref{fig:offload-spectrum} shows, there is a wide range between the two ends
of the spectrum. Depending on the current network conditions, partial-offload
mobile devices can utilize a combination of onboard and remote computational
resources that results in optimal performance.

Mobile devices can achieve this through an optimal partitioning of applications
into local and remote components. Flinn et al tackled the issue of developing
applications that can support devices with different capabilities and dynamic
execution conditions with a self-tuning remote execution system. The system,
called \textit{Spectra}, continuously monitors the application's resource
supply and demand and suggests which components of an application should be
executed remotely, taking into account factors such as performance, energy
usage, and fidelity \cite{flinn2002}. This work requires the application
developer to partition the application, specifying application components
that could benefit from remote execution.

\subsection{Offloading Shaping}
\label{sec:offload-shaping}

Hu et al \cite{hu2015} showed that there is value to doing additional onboard
computation when offloading, which was not originally part of the application.
The approach, called \textit{offload shaping}, attempts to conserve wireless
bandwidth and energy through the process of \textit{early discard}. The process
involves selectively sending inputs for offload computation based on an
application-specific metric of value assigned to each input.  In the case of
video analytics, for instance, onboard computation can detect blurry frames and
not transmit them to the cloudlet. Hu et al showed that object recognition does
not perform well on blurry frames, and so it is a waste of bandwidth and energy
to transmit these blurry frames to a cloudlet for computation.

\section{A Partial Offload Pipeline for SteelEagle}

SteelEagle drones can currently be placed in the ``pure offload'' part of the
spectrum in \cref{fig:offload-spectrum}, since they do not perform any on-board
computation other than the encoding of the raw camera feed to an RTSP H.264
video stream that is transmitted to a cloudlet. As discussed in
\cref{sec:deciding-offloading-strategy}, a pure offload strategy does not
support disconnected operation, and impacts the application considerably as
network conditions degrade.

\begin{figure}[htbp]
\centerline{\includegraphics[width = .5\textwidth]{figs/voxl2.png}}
\caption{Modal AI VOXL 2}
\label{fig:voxl2}
\end{figure}

Replacing the Onion Omega 2 with the Modal AI VOXL 2 (\cref{fig:voxl2}) as the
communications relay in the SteelEagle pipeline will allow us to pursue a
partial offloading strategy that allows SteelEagle drones to gracefully react
to changes in network conditions. Intended for use as an AI autopilot in custom
drones, the VOXL 2 features a Qualcomm QRB5165 processor and a PX4 flight
controller. We do not utilize the PX4 flight controller on the VOXL.
\Cref{tab:voxl2-specs} provides information about the computational resources
available on the VOXL 2.

\begin{table}[htbp]
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{} & \textbf{ModalAI VOXL 2}\\
        \midrule
        \textbf{Architecture} & 64-bit ARM\\
        \textbf{CPU} & Qualcomm Kyro 585 (7nm, released Dec 2019)\\
                     & 1 x 2.84 GHz high-performance core\\
                     & 3 x 2.42 GHz high performance cores\\
                     & 4 x 1.80 GHz low-performance cores\\
        \textbf{ISP} & Qualcomm Spectra 480\\
        \textbf{GPU} & Qualcomm Adreno 650, with support for OpenCL\\
        \textbf{DSP} & Qualcomm Hexagon 698\\
        \textbf{Memory} & 8 GB\\
        \textbf{Weight} & 16 grams\\
        \textbf{Power consumption} & 4-5 W under high load\\
        \textbf{Operating system} & Ubuntu 18.04\\
        \bottomrule
    \end{tabular}
    \caption{Technical specifications of the Modal AI VOXL 2 system-on-a-chip}
    \label{tab:voxl2-specs}
\end{table}

The VOXL 2 has support for running Google's Lite Runtime (LiteRT) machine
learning models, formely known as TensorFlow Lite (TFLite). Trained TensorFlow
and PyTorch models can be converted to LiteRT models by using techniques such
as quantization and pruning \cite{jacob2017}. Float16 quantization reduces the
floating-point precision of the model's weights to 16-bits, and full integer
quantization converts them to integers. This reduces the model's size, memory
usage and inference latency, making it more suitable for mobile device
inferencing. Float16 quantization, for instance, reduces the size of the model
by half and causes minimal loss in accuracy. However, GPU acceleration of
float16-quantized models requires half-precision floating point support (FP16)
\cite{ho2017}.

\subsection{Mapping to the OODA Loop Framework}

\begin{figure}[htbp]
\definecolor{observe-color}{RGB}{175,208,149}
\definecolor{orient-color}{RGB}{255, 255, 166}
\definecolor{decide-color}{RGB}{255,170,149}
\definecolor{act-color}{RGB}{224,194,205}
\centering
\includegraphics[width = .7\textwidth]{figs/fig-voxl-ooda-loop-crop.pdf}
\begin{comment}
\begin{tikzpicture}
    \draw[fill=observe-color] (1.0,0) rectangle (1.3,0.3);
    \node[right] at (1.3,0.15) {\small Observe};

    \draw[fill=decide-color] (2.9,0) rectangle (3.2,0.3);
    \node[right] at (3.2,0.15) {\small Orient \& Decide};

    \draw[fill=act-color] (6.1,0) rectangle (6.4,0.3);
    \node[right] at (6.4,0.15) {\small Act};
\end{tikzpicture}
\end{comment}
\begin{tikzpicture}
    % Observe box
    \draw[fill=observe-color] (1.0,0) rectangle (1.3,0.3);
    \node[right] at (1.3,0.15) {\small Observe};

    % Orient box with yellow hatching
    \draw[fill=white, pattern=north east lines, pattern color=yellow]
        (2.9,0) rectangle (3.2,0.3);
    \node[right] at (3.2,0.15) {\small Orient};

    % Decide box
    \draw[fill=decide-color] (4.5,0) rectangle (4.8,0.3);
    \node[right] at (4.8,0.15) {\small Decide};

    % Act box
    \draw[fill=act-color] (6.2,0) rectangle (6.5,0.3);
    \node[right] at (6.5,0.15) {\small Act};
\end{tikzpicture}
\caption{Mapping the VOXL 2 pipeline to the OODA loop}
\label{fig:voxl2-ooda-loop-mapping}
\end{figure}

\Cref{fig:voxl2-ooda-loop-mapping} depicts the new pipeline with the VOXL 2.
In the existing SteelEagle architecture, as described in
\cref{sec:steeleagle-bg}, video decoding is performed on the cloudlet since all
video stream network packets are forwarded to the cloudlet using the Onion
Omega as a network gateway. To perform onboard computation on the VOXL 2, we
must obtain individual frames from the drone's video stream.  This requires
decoding the H.264 video stream on the VOXL 2, which commences the Orient
phase.  The VOXL pipeline has an inner and outer Orient loop:
\begin{itemize}
    \item \textbf{Inner loop}: Corresponds to components Orient$_e$ and
        Orient$_f$, which involve analysis of the decoded frame and planning
        on the VOXL
    \item \textbf{Outer loop}: Corresponds to components Orient$_w$ through
        Orient$_z$. Involves offloaded analysis of the decoded frame and
        planning on the cloudlet
\end{itemize}

The outer loop involves offloading to the cloudlet, which incurs propogation
and transmission delay to and from the cloudlet, Orient$_w$ and Orient$_z$. The
cloudlet, being a stationary device, can run more computationally-intensive
analysis and planning phases which can yield more accurate results in less
time.

To conserve bandwidth, the outer loop is not always exercised.  The pipeline
can leverage offload shaping techniques (\cref{sec:offload-shaping}) to
determine if the frame should be discarded. Then, a remote execution system
(\cref{sec:offloading-dynamic-nature}) can determine if processing should be
offloaded. Once frame processing is complete, the pipeline determines whether
any drone actuation needs to be performed, and generates the corresponding
command.

\subsection{Onboard Computation Design}

\begin{figure}[htbp]
\centering
\includegraphics[width = .9\textwidth]{figs/onboard-design-crop.pdf}
\begin{captext}
Components in red are developed in Python, and components in green in C++
\end{captext}
\caption{VOXL 2 Onboard Computation Pipeline}
\label{fig:voxl2-onboard-pipeline}
\end{figure}

The VOXL 2 supports the Open Computing Language (OpenCL) standard, which
assists in writing parallel programs which leverage its hardware accelerators,
including its GPU. The VOXL 2 ships with a software suite that includes various
programs designed to be run as \texttt{systemd} services. One of these
programs, VOXL TFlite Server, performs hardware-accelerated machine learning
inference. The inputs and outputs of the programs are sent through an
inter-process communication library called \texttt{libmodalpipe}, which uses
Unix pipes. \texttt{libmodalpipe} includes data structures that represent data
items such as camera image metadata and the results of inference. For example,
\texttt{camera\_image\_metadata\_t} specifies image metadata such as width,
height, size and format of the image, and \texttt{ai\_detection\_t} includes
information relevant to object detection models, such as detected classes,
confidence levels, and bounding boxes.

VOXL TFlite Server is designed to take the drone's camera data continuously as
input through a Unix pipe and output detections through another Unix pipe. Any
service interesting in consuming the inference results can register as a client
to the output pipe. We can utilize this framework to perform onboard inference,
publishing frames requiring inference to a pipe configured as input to VOXL
TFLite Server.

The VOXL software ecosystem is targeted for C/C++ programs. However, the
SteelEagle pipeline is currently developed in Python. As a result, a C++ proxy
is required that can receive frames from the SteelEagle program and forward
them to the VOXL TFLite Server for inference.  As
\cref{fig:voxl2-onboard-pipeline} shows, an onboard compute module was
developed that communicates with SteelEagle using the ZeroMQ messaging library,
listening for inference requests. ZeroMQ has bindings available for popular
programming languages, making it suitable for this use case. A SteelEagle stub
provides a convenience API that generates a \texttt{protobuf} message that is
sent to the onboard compute module. The \texttt{protobuf} message includes the
contents of the frame as an RGB image along with frame metadata such as width
and height. The onboard compute client generates the
\texttt{camera\_image\_metadata\_t} structure that VOXL TFLite Server expects,
and sends it to its input pipe along with the frame data.

Once the VOXL TFLite Server receives an input, it publishes one
\texttt{ai\_detection\_t} data structure to the output pipe for each detection
on a given frame. There is no metadata published that specifies the number of
detections to expect. This works well if a consumer wants a live stream of
object detections. However, in our case, we only want to reply to the
SteelEagle program once we receive all detections for a given frame. To achieve
this, the VOXL TFLite Server program was modified to send a delimiter
\texttt{ai\_detection\_t} data structure once all the detections have been sent
for a given frame. Once this delimiter is received, the onboard compute module
adds all the detections to a single \texttt{protobuf} message, and sends it
back to the SteelEagle program.

\subsection{Adaptive Transmission Rate in the VOXL 2 Pipeline}

In the VOXL pipeline, frames are decoded on the VOXL instead of the cloudlet.
This means that if the remote execution system decides to offload computation
to the cloudlet, it must send individual frames to the cloudlet. This increases
bandwidth requirements substantially. Previously, a bandwidth of 5 Mbps was
sufficient to send the video stream to the cloudlet. Individual 720p frames
average about 180 KB when encoded as JPEG, requiring a bandwidth of 43.2 Mbps
to send 30 frames per second.

As \cref{tab:voxl2-offload-performance} shows, attempting to send 30 frames per
second on the VOXL 2, without any flow control, overwhelms the receiver since
the VOXL 2 does not have sufficient bandwidth.  Only a frame rate of 12.58 per
second is achievable when sending frames at full-fidelity. We also observe
queueing delay leading to significantly higher latency when attempting to send
full-fidelity frames.  Reducing frame size allows us to achieve better
throughput, and gradually reduce latency as network queuing delay and
transmission delay decrease.

\begin{table}[htbp]
    \centering
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{Frame details} & \textbf{Frame size (KB)} & \textbf{Avg. Latency (ms)} & \textbf{Achievable FPS}\\
        \midrule
        720p & 180 & 934$\pm$\small{30.47} & 12.58\\
        720p, 80\% JPG quality  & 98 & 600$\pm$\small{65.76} &28.63\\
        720p, 70\% JPG quality & 79 & 304$\pm$\small{12.82} & 29.95\\
        360p & 79 & 307$\pm$\small{20.60}  &29.89 \\
        360p, 80\% JPG quality & 38 &288$\pm$\small{13.96} & 29.89\\
        360p, 70\% JPG quality & 31 & 286$\pm$\small{17.26} & 29.85\\
        \bottomrule
\end{tabular}\\
\begin{captext}
``Achievable FPS'' is calculated at the cloudlet
by considering the number of frames that are received in a given time frame
\end{captext}
\caption{Offloading Performance on the VOXL 2 (30 FPS)}
\label{tab:voxl2-offload-performance}
\end{table}

Restricting our transmission rate to 10 frames per second,
\cref{tab:voxl2-offload-performance-10-fps,}, we no longer observe queuing delay.
Even with full-fidelity frames we are able to achieve the desired frame rate.

\begin{table}[htbp]
    \centering
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{Frame details} & \textbf{Frame size (KB)} & \textbf{Avg. Latency (ms)} & \textbf{Achievable FPS}\\
        \midrule
        720p & 180 & 363$\pm$\small{22.06} & 9.98\\
        720p, 80\% JPG quality  & 98 & 343$\pm$\small{26.04} &10\\
        720p, 70\% JPG quality & 79 & 340$\pm$\small{46.47} & 9.96\\
        360p & 79 & 312$\pm$\small{23.48}  &9.95 \\
        360p, 80\% JPG quality & 38 &301$\pm$\small{27.40} & 9.97\\
        360p, 70\% JPG quality & 31 & 289$\pm$\small{18.81} & 9.98\\
        \bottomrule
\end{tabular}
\begin{captext}
``Achievable FPS'' is calculated at the cloudlet
by considering the number of frames that are received in a given time frame
\end{captext}
\caption{Offloading Performance on the VOXL 2 (10 FPS)}
\label{tab:voxl2-offload-performance-10-fps}
\end{table}

These results show that an adaptive transmission based on current network
conditions is essential in the VOXL pipeline. Setting a low static frame rate
up front can leave performance on the table if the available bandwidth can
sustain a higher frame rate. Similarly, the available the bandwidth might
not even be sufficient to sustain the static frame rate set, which is
highly possible in commercial cellular networks.

Chen \cite{zchenthesis} approached this issue through the use of a token-bucket
filter, which limits the number of frames allowed to be in flight at any given
time. The mechanism replenishes tokens by having the receiver send back a token
when a frame has been received. \Cref{tab:token-latency} shows the results when
using a token-bucket filter in the VOXL pipeline, sending frames at full-fidelity
(720p).

\begin{table}[htbp]
    \centering
    \begin{tabular}{@{}ccc@{}}
        \toprule
        \textbf{Total Tokens} & \textbf{Average Latency (ms)} & \textbf{Frame Rate (FPS)}\\
        \midrule
        1 &  398$\pm$\small{32.57} & 6.03\\
        2 & 443$\pm$\small{40.81} & 11.42\\
        3 & 506$\pm$\small{34.93} & 11.11\\
        \bottomrule

    \end{tabular}
    \caption{Drone-to-cloudlet Latency with Different Token Counts}
    \label{tab:token-latency}
\end{table}

In Chen's approach, the number of tokens used are not dynamic. According to
Chen, a small token count minimizes latency at the expense of throughput and
resource utilization. This is indeed the observation we make: a token count of
one results in a mean latency of 398 ms, and going up to three tokens, which
allows up to three frames to be in flight at a time, results in a mean latency
of 506 ms. A set-and-forget approach that uses just one token optimizes latency,
but can lead to a lower throughput than otherwise possible, especially if
network bandwidth improves.

Our observations show the need for future work to achieve an optimal balance of
latency and bandwidth based on the current network conditions. A dynamic token
limit, for instance, can help achieve this. Obtaining real-time telemetry that
indicates the end-to-end transmission time can inform a decision to dynamically
alter token counts.

These results also emphasize the fact that bandwidth is a precious and limited
resource that can vary over time. \Cref{fig:voxl-bw-measurement} shows bandwidth measurement obtained using
\texttt{iperf} on the VOXL using a commercial 5G provider. The mean of 11.28 Mbps
is decent, but comes with a standard deviation of 9.34 Mbps and a p1 of 0 Mbps!

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/bw_out.pdf}\\
    \small{Mean: 11.28$\pm$9.34~Mbps\; p1: 0~Mbps}\\
    \caption{Bandwidth Measurement on the VOXL Using 5G Modem}
    \label{fig:voxl-bw-measurement}
\end{figure}

\subsection{OODA Loop of VOXL 2 SteelEagle Pipeline}

Using terminology from \cref{fig:voxl2-ooda-loop-mapping}, we consider the
performance of each component of the VOXL pipeline.

\subsubsection*{Observe$_{ab}$}

The Observe$_{ab}$ component of the OODA loop in the new pipeline is the same as in
the case of the Onion Omega (\cref{sec:onion-observe-ab}), including on-drone
sensing and pre-processing, which involves generation of an RTSP H.264 video
stream from the drone camera's raw video frames. We use the same drone,
the Parrot Anafi, for the VOXL pipeline. As before, this component has a mean
of 253 ms, with a standard deviation of 12 ms, and a p99 of 277 ms.

\subsubsection*{Observe$_{c}$}

Observe$_c$ involves a short Wi-Fi segment from the drone to the VOXL 2, with
the drone positioned centimeters away.  Measurements obtained using the
\texttt{ping} tool show that the round-trip latency has a mean of 10.33 ms,
with a standard deviation of 1.56 ms. This provides us with an estimate of
one-way propogation delay of 5 ms. Bandwidth on this segment cannot be measured
since the drone is a black box.

\begin{comment}
\Cref{fig:voxl2-decoding-performance} shows the latency measurements. The
latency distribution has a mean of 262 ms, with a standard deviation of 10.5 ms
and a p99 of 282 ms. This is comparable to the Observe$_{ab}$ measurements from
the Onion Omega pipeline.
\end{comment}

\subsubsection*{Orient$_{d}$}

\begin{figure}[htb]
    \centering
    \includegraphics[width = .4\textwidth]{figs/voxl-decoding-time.pdf}\\
\small{Mean: 54.84$\pm$11.59~ms\; p99: 76.47~ms}\\
\caption{Orient$_d$ Measurements}
\label{fig:voxl2-decoding-latency}
\end{figure}

Orient$_d$ involves the decoding of the RTSP video stream generated by the
drone.  \Cref{fig:voxl2-decoding-latency} shows the measurements of the video
decoding. The latency distribution has a mean of 55 ms, with a standard
deviation of 11.6 ms, and a p99 of 76.5 ms. This is comparable to the decoding
latency we obtained for the Onion Omega pipeline
(\cref{sec:onion-orient-decide-d}), which had a mean of 32.26 ms, with a
standard deviation of 12.75 ms and a p99 of 59.08 ms.

\subsubsection*{Orient$_{e}$}

\begin{figure}[htbp]
\centerline{\includegraphics[width = 0.4\textwidth]{figs/onboard-inference-hist.pdf}}
\centering
Mean: 60.58$\pm$6.98~ms\; p99: 80~ms\\
\caption{Orient$_e$ Measurements Using a Float16-quantized YOLOv5 Model}
\label{fig:voxl2-inference-hist}
\end{figure}

Orient$_e$ corresponds to Stage 2 of the Onion Omega pipeline, discussed in
\cref{sec:onion-orient-decide-d}. \Cref{fig:voxl2-inference-hist} shows
measurements for running a machine learning model on the VOXL. We use a
float16-quantized YOLOv5 model for inference, which is used for detecting
objects. We obtain a mean latency of 60.58 ms, with a standard deviation of
6.98 ms and a p99 of 80 ms.

\subsubsection*{Orient$_{f}$}

Orient$_f$ involves processing on the VOXL with the inference results obtained
from Orient$_e$. The performance of this component is very application-specific,
so we ignore it for the purposes of this assessment.

\subsubsection*{Orient$_{w}$}

Orient$_w$ involves the latency for sending a frame to the cloudlet for
offloading computation. \Cref{fig:voxl2-5g-latency} shows that this latency
has a mean of 53.64 ms, with a standard deviation of 9.65 ms and a p99 of
83.03 ms.

\begin{figure}[htbp]
    \centering
    \includegraphics[width = .4\textwidth]{figs/voxl-5g-latency.pdf}\\
\small{Mean: 53.64$\pm$9.65~ms\; p99: 83.03~ms}\\
\caption{Orient$_w$ Measurements}
\label{fig:voxl2-5g-latency}
\end{figure}

\subsubsection*{Orient$_{x}$}

\Cref{tab:yolo-model-stats} shows the performance of various YOLO models on a
cloudlet. The mean latency varies from 28 ms for the smallest YOLO model to 42
ms for the largest. The latency of onboard inference, Orient$_e$, is greater
than that of the largest YOLO model on the cloudlet, which emphasizes the
mobility penalty that the VOXL incurs.

\begin{table}[htbp]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Latency} & \textbf{Throughput} & \textbf{mAP} \\
 & (ms) & (fps) & \\
\midrule
YOLOv5s  & 28 & 25 & 56.8\\
YOLOv5m  & 37 & 20 & 64.1\\
YOLOv5l  & 42 & 20 & 67.3\\
\bottomrule
\end{tabular}\\
\begin{captext}
  The inference and throughput were obtained on our
  cloudlet.  The mean average precision~(mAP)
  is from the YOLO documentation.
\end{captext}
\vspace{-0.1in}
\caption{YOLOv5 Performance on Cloudlet (from \cite{bala2025})}
\label{tab:yolo-model-stats}
\end{table}

\subsubsection*{Orient$_{y}$}

Orient$_y$ performs the same function as Orient$_f$ on the VOXL.  It involves
processing with the inference results obtained from Orient$_x$. The performance
of this component is very application-specific, so we ignore it for the
purposes of this assessment.

\subsubsection*{Orient$_{z}$}

Orient$_z$ involves a 5G segment from the cloudlet to the VOXL 2. We re-use
the measurements from Orient$_{w}$ for this component.

\subsubsection*{Act$_{h}$}

Act$_h$ involves a Wi-Fi segment from the VOXL 2 to the drone. We obtain the
same measurement for Act$_h$ as Orient$_c$, with a estimate of 5 ms obtained
using the \texttt{ping} utility.

\subsubsection*{Act$_{ij}$}

Act$_{ij}$ is the same as Act$_{fg}$ in the Onion Omega pipeline, discussed in \cref{sec:onion-act-fg}, involving drone post-processing and actuation. For the actuation of the Parrot Anafi drone's gimbal, it has a mean of 173 ms and a standard deviation of 15 ms.

